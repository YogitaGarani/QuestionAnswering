{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3Kf0jYlkd9c"
      },
      "outputs": [],
      "source": [
        "# Installations\n",
        "! pip install -q transformers[torch] datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m5zLp6gdHP7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "squad = load_dataset(\"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "squad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3212486a-8fd3-4070-c7ab-d3d129a0adef",
        "id": "KjpwWfJRdHP8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "fa6869d3-e083-4bb5-8dcd-6a5f4db5761f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YXE3UFzdHP7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id : 5733bed24776f41900661188\n",
            "title : University_of_Notre_Dame\n",
            "context : The university is the major seat of the Congregation of Holy Cross (albeit not its official headquarters, which are in Rome). Its main seminary, Moreau Seminary, is located on the campus across St. Joseph lake from the Main Building. Old College, the oldest building on campus and located near the shore of St. Mary lake, houses undergraduate seminarians. Retired priests and brothers reside in Fatima House (a former retreat center), Holy Cross House, as well as Columba Hall near the Grotto. The university through the Moreau Seminary has ties to theologian Frederick Buechner. While not Catholic, Buechner has praised writers from Notre Dame and Moreau Seminary created a Buechner Prize for Preaching.\n",
            "question : Where is the headquarters of the Congregation of the Holy Cross?\n",
            "answers : {'text': ['Rome'], 'answer_start': [119]}\n"
          ]
        }
      ],
      "source": [
        "example = squad['train'][10]\n",
        "for key in example:\n",
        "    print(key, \":\", example[key])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx1Ec4kWZp22",
        "outputId": "4a27e8b0-f170-4c0a-a8fe-4c2ce7139b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_train_features(examples):\n",
        "\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=384,\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "    #print(offset_mapping[0])\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "id": "EmWKtSV4Dd1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to our data\n",
        "tokenized_datasets = squad.map(prepare_train_features, batched=True, remove_columns=squad[\"train\"].column_names)\n"
      ],
      "metadata": {
        "id": "7Jffo-X1DgOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Tflh0n-ELha",
        "outputId": "f3286155-1639-4dd0-98d1-aceed3d8e74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTzo-yBbEGfa",
        "outputId": "3dcfd25d-a849-483c-e6eb-f695e3cd11eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
              "        num_rows: 88524\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
              "        num_rows: 10784\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"finetune-BERT-squad\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=4e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.999,\n",
        "    adam_epsilon=1e-08\n",
        ")"
      ],
      "metadata": {
        "id": "e3_L4CoJEj7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ],
      "metadata": {
        "id": "ugAYPewYFMWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"].select(range(1000)),\n",
        "    eval_dataset=tokenized_datasets[\"validation\"].select(range(100)),\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "pUILHrRtExwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c7c975-3a98-47f6-87c1-334d1ee3da89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the trainer\n",
        "import torch\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "GUnFJJ64E1hS",
        "outputId": "a2c9e65c-c54a-4b89-e7a0-d9c2b2b9efde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 03:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.370839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.043788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.296517</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=375, training_loss=2.0398819986979166, metrics={'train_runtime': 226.1114, 'train_samples_per_second': 13.268, 'train_steps_per_second': 1.658, 'total_flos': 587917702656000.0, 'train_loss': 2.0398819986979166, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instance = squad['train'][20]\n",
        "context = instance['context']\n",
        "question = instance['question']"
      ],
      "metadata": {
        "id": "pEoNS2GvwmW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "id": "ScoA3xgXb1lH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "305958f1-adff-4a9e-9c9f-5aa98490acb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"All of Notre Dame's undergraduate students are a part of one of the five undergraduate colleges at the school or are in the First Year of Studies program. The First Year of Studies program was established in 1962 to guide incoming freshmen in their first year at the school before they have declared a major. Each student is given an academic advisor from the program who helps them to choose classes that give them exposure to any major in which they are interested. The program also includes a Learning Resource Center which provides time management, collaborative learning, and subject tutoring. This program has been recognized previously, by U.S. News & World Report, as outstanding.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instance['answers']"
      ],
      "metadata": {
        "id": "tYweAKSRZaUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d5e872-df2c-442f-997a-441e01b0a008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ['Learning Resource Center'], 'answer_start': [496]}"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "given_answer = instance['answers']['text'][0]  # Assuming the first answer is the correct one\n",
        "given_answer_start = instance['answers']['answer_start'][0]\n",
        "given_answer, given_answer_start"
      ],
      "metadata": {
        "id": "QRWHNFgmY-UR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09cd1a3a-67e0-4645-bd35-1f5b83889c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Learning Resource Center', 496)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "inputs = tokenizer(question, context, return_tensors='pt', max_length=512, truncation=True)"
      ],
      "metadata": {
        "id": "ZweyIoxmZroB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "YoYA_nRfHZVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f11b0588-182b-42e4-a64b-f0a5d07b8889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {k: v.to(device) for k, v in inputs.items()}"
      ],
      "metadata": {
        "id": "9a3xrBUsHfhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model's output\n",
        "with torch.no_grad():\n",
        "    output = model(**inputs)"
      ],
      "metadata": {
        "id": "B8a3e1vRZ0x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted answer\n",
        "start_idx = torch.argmax(output.start_logits)\n",
        "end_idx = torch.argmax(output.end_logits)\n",
        "\n",
        "predicted_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_idx:end_idx + 1]))\n"
      ],
      "metadata": {
        "id": "hhOQJmHZZ2Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_answer, start_idx, end_idx, start_idx.item(), end_idx.item()"
      ],
      "metadata": {
        "id": "AgIKXzAcaELZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d549f068-f945-4248-8121-60fa373d4d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('learning resource center',\n",
              " tensor(111, device='cuda:0'),\n",
              " tensor(113, device='cuda:0'),\n",
              " 111,\n",
              " 113)"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = (predicted_answer.lower() == given_answer.lower())\n",
        "evaluation = 'Correct' if correct else f'Incorrect (Predicted: {predicted_answer}, Given: {given_answer})'\n",
        "\n",
        "print(evaluation)"
      ],
      "metadata": {
        "id": "IiKfd3s0aG2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7473e4ff-08d6-4ff3-d613-48317f658599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "def evaluate_instance(instance, device):\n",
        "    context = instance['context']\n",
        "    question = instance['question']\n",
        "    given_answer = instance['answers']['text'][0]  # Assuming the first answer is the correct one\n",
        "\n",
        "    # Tokenize the data\n",
        "    inputs = tokenizer(question, context, return_tensors='pt', max_length=512, truncation=True)\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Apply the BERT model\n",
        "    with torch.no_grad():  # No need to calculate gradients\n",
        "        output = model(**inputs)\n",
        "\n",
        "    # Get the predicted answer\n",
        "    start_idx = torch.argmax(output.start_logits)\n",
        "    end_idx = torch.argmax(output.end_logits)\n",
        "    predicted_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_idx:end_idx + 1]))\n",
        "\n",
        "    return predicted_answer.lower(), given_answer.lower()\n",
        "\n",
        "# Evaluate a number of instances\n",
        "correct_count = 0\n",
        "f1_scores = []\n",
        "recalls = []\n",
        "em_scores = []\n",
        "reciprocal_ranks = []\n",
        "\n",
        "total_count = 100\n",
        "\n",
        "for i in tqdm(range(total_count)):\n",
        "    predicted_answer, given_answer = evaluate_instance(squad['train'][i], device)\n",
        "\n",
        "    # Compute F1 score\n",
        "    common = collections.Counter(predicted_answer.split()) & collections.Counter(given_answer.split())\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        f1_scores.append(0)\n",
        "    else:\n",
        "        precision = 1.0 * num_same / len(predicted_answer.split())\n",
        "        recall = 1.0 * num_same / len(given_answer.split())\n",
        "        f1_scores.append((2 * precision * recall) / (precision + recall))\n",
        "\n",
        "    # Compute recall\n",
        "    recalls.append(num_same / len(given_answer.split()))\n",
        "\n",
        "    # Compute EM score\n",
        "    em_scores.append(int(predicted_answer == given_answer))\n",
        "\n",
        "    # Compute reciprocal rank\n",
        "    reciprocal_rank = 0\n",
        "    if num_same > 0:\n",
        "        predicted_tokens = predicted_answer.split()\n",
        "        given_tokens = given_answer.split()\n",
        "        ranks = [next((i for i, v in enumerate(predicted_tokens) if v == token), 0) + 1 for token in given_tokens]\n",
        "        reciprocal_rank = np.mean([1 / rank for rank in ranks if rank > 0])\n",
        "    reciprocal_ranks.append(reciprocal_rank)\n",
        "\n",
        "# Calculate and output the metrics\n",
        "f1_score = np.mean(f1_scores)\n",
        "recall = np.mean(recalls)\n",
        "em_score = np.mean(em_scores)\n",
        "mrr = np.mean(reciprocal_ranks)\n",
        "\n",
        "print(f'F1 Score: {f1_score:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'EM Score: {em_score * 100:.2f}%')\n",
        "print(f'MRR: {mrr:.4f}')\n"
      ],
      "metadata": {
        "id": "xlya_DCmbjUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec00139a-539c-4abc-9e65-27a842dc0082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 60.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.7214\n",
            "Recall: 0.7416\n",
            "EM Score: 63.00%\n",
            "MRR: 0.5692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "32k95cSOhFdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_nfmEP3_gyTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kh-t0j9hgDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDJQMWyvx6Jl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}